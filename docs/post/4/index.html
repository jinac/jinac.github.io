<!DOCTYPE html>
<html lang="en">
  <head>
    
      <title>
        Creating a Face Multitask Dataset ::
        John Inacay â€” Computer Vision and Machine Learning Engineer // Composer and Multi-Instrumentalist
      </title>
    
    <meta http-equiv="content-type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<meta
  name="description"
  content="Overview With current 3D animation software, you no longer need large studio resources to create your own short films. Current hardware and software can make real-time animation possible as seen with the popularity of V-tubers. As it&amp;rsquo;s easier to create, motion capture should be making its way to the bedroom creator. I&amp;rsquo;m interested in where deep learning can help with this. Focusing on the V-tuber aspect, facial capture seems to be a strong starting point, especially when there are models for different aspects of facial capture like head pose and facial landmarks."
/>
<meta
  name="keywords"
  content=""
/>
<meta name="robots" content="noodp" />
<link rel="canonical" href="https://jinac.github.io/post/4/" />





<link rel="stylesheet" href="https://jinac.github.io/assets/style.css" />

<link rel="stylesheet" href="https://jinac.github.io/style.css" />


<link
  rel="apple-touch-icon-precomposed"
  sizes="144x144"
  href="https://jinac.github.io/img/apple-touch-icon-144-precomposed.png"
/>
<link rel="shortcut icon" href="https://jinac.github.io/img/favicon.png" />


<link href="https://jinac.github.io/assets/fonts/Inter-Italic.woff2" rel="preload" type="font/woff2" as="font" crossorigin="">
<link href="https://jinac.github.io/assets/fonts/Inter-Regular.woff2" rel="preload" type="font/woff2" as="font" crossorigin="">
<link href="https://jinac.github.io/assets/fonts/Inter-Medium.woff2" rel="preload" type="font/woff2" as="font" crossorigin="">
<link href="https://jinac.github.io/assets/fonts/Inter-MediumItalic.woff2" rel="preload" type="font/woff2" as="font" crossorigin="">
<link href="https://jinac.github.io/assets/fonts/Inter-Bold.woff2" rel="preload" type="font/woff2" as="font" crossorigin="">
<link href="https://jinac.github.io/assets/fonts/Inter-BoldItalic.woff2" rel="preload" type="font/woff2" as="font" crossorigin="">


<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Creating a Face Multitask Dataset"/>
<meta name="twitter:description" content="Manufacturing a datset and building a dataset loader"/>



<meta property="og:title" content="Creating a Face Multitask Dataset" />
<meta property="og:description" content="Manufacturing a datset and building a dataset loader" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://jinac.github.io/post/4/" />
<meta property="article:published_time" content="2021-06-01T00:00:00+00:00" />
<meta property="article:modified_time" content="2021-06-01T00:00:00+00:00" /><meta property="og:site_name" content="John Inacay" />






  </head>
  <body class="dark-theme">
    <div class="container">
      <header class="header">
  <span class="header__inner">
    <a
  href="/"
  class="logo"
  style="text-decoration: none;"
>
  
    <span class="logo__mark"><svg xmlns="http://www.w3.org/2000/svg" class="greater-icon" viewBox="0 0 44 44">
  <path fill="none" d="M15 8l14.729 14.382L15 35.367" />
</svg>
</span>
    <span class="logo__text"
      >John Inacay</span
    >
    <span class="logo__cursor"></span>
  
</a>

    <span class="header__right">
      
        <nav class="menu">
  <ul class="menu__inner menu__inner--desktop">
    
      
        
          <li><a href="/about/">About</a></li>
        
      
        
          <li><a href="/post/">Blog</a></li>
        
      
        
          <li><a href="/music/">Music</a></li>
        
      
        
          <li><a href="/resume/">Resume</a></li>
        
      
      
      
  </ul>

  <ul class="menu__inner menu__inner--mobile">
    
      
        <li><a href="/about/">About</a></li>
      
    
      
        <li><a href="/post/">Blog</a></li>
      
    
      
        <li><a href="/music/">Music</a></li>
      
    
      
        <li><a href="/resume/">Resume</a></li>
      
    
  </ul>
</nav>

        <span class="menu-trigger">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
            <path d="M0 0h24v24H0z" fill="none" />
            <path d="M3 18h18v-2H3v2zm0-5h18v-2H3v2zm0-7v2h18V6H3z" />
          </svg>
        </span>
      
      <span class="theme-toggle">
        <svg
  class="theme-toggler"
  width="24"
  height="24"
  viewBox="0 0 48 48"
  fill="none"
  xmlns="http://www.w3.org/2000/svg"
>
  <path
    d="M22 41C32.4934 41 41 32.4934 41 22C41 11.5066 32.4934 3 22
  3C11.5066 3 3 11.5066 3 22C3 32.4934 11.5066 41 22 41ZM7 22C7
  13.7157 13.7157 7 22 7V37C13.7157 37 7 30.2843 7 22Z"
  />
</svg>

      </span>
    </span>
  </span>
</header>


      <div class="content">
        
  
  

  <div class="post">
    <h1 class="post-title">Creating a Face Multitask Dataset</h1>
    <div class="post-meta">
      
        <span class="post-date">
          2021-06-01
        </span>

        
          
        
      

      


      
    </div>

    

    

    <div class="post-content">
      
        <h2>Table of Contents</h2>
        <aside class="table-of-contents"><nav id="TableOfContents">
  <ul>
    <li><a href="#format">Format</a></li>
  </ul>
</nav></aside>
      
      <h1 id="overview">Overview</h1>
<p>With current 3D animation software, you no longer need large studio resources to create your own short films. Current hardware and software can make real-time animation possible as seen with the popularity of V-tubers. As it&rsquo;s easier to create, motion capture should be making its way to the bedroom creator. I&rsquo;m interested in where deep learning can help with this. Focusing on the V-tuber aspect, facial capture seems to be a strong starting point, especially when there are models for different aspects of facial capture like head pose and facial landmarks. However, these models were trained separately. I am interested in seeing if we can further optimize for different related facial motion capture tasks in a deep learning approach. But before I can train, I need a dataset.</p>
<h1 id="dataset">Dataset</h1>
<p>It&rsquo;s costly to label a dataset myself, so I started from the <a href="https://www.cs.tau.ac.il/~wolf/ytfaces/">YouTube Faces DB</a>. More specifically, I used a processed <a href="https://www.kaggle.com/selfishgene/youtube-faces-with-facial-keypoints">Kaggle dataset</a> as the real starting point to get many frames containing face detections with facial keypoints. I further add other outputs like head pose and gaze vector estimation, adapting an <a href="https://github.com/mmphego/computer-pointer-controller">OpenVINO-based gaze detection code</a>.</p>
<p>Some frames failed in adding the data labels we want, but we just drop them. We still have a sizeable dataset to work with (16 GB zipped).</p>
<h2 id="format">Format</h2>
<p>The data itself are .npz files. Each npz file contains numpy arrays for a single video. Once loaded using numpy, the npz object contains the following objects:</p>
<table>
<thead>
<tr>
<th>Field name in npz object</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>colorImages</td>
<td>the rgb frames for given video</td>
</tr>
<tr>
<td>boundingBox</td>
<td>the x-y coordinates of each corner a bounding box for the detected face</td>
</tr>
<tr>
<td>landmarks2D</td>
<td>the 2D 68 facial landmarks for each frame</td>
</tr>
<tr>
<td>landmarks3D</td>
<td>the 3D 68 facial landmarks for each frame; the x-y coordinates match the 2D</td>
</tr>
<tr>
<td>headPose</td>
<td>3-dimensional head pose angles in format of [yaw, pitch, roll]</td>
</tr>
<tr>
<td>gazeVector</td>
<td>3-dimensional vector estimating gaze direction in format of [x, y, z]</td>
</tr>
</tbody>
</table>
<h1 id="dataloader">Dataloader</h1>
<p>Now that we have data and I&rsquo;m intending to use pytorch, we can make use of pytorch&rsquo;s dataset class to feed data in my future training code. In the dataloader, we also intend to get more out of our data by adding augmentations. Also, with a subset of our data being affected when we apply spatial augmentations (rotations, shifts, etc.), I had to account for the changes, particularly with the landmarks. If we apply a horizontal flip, the landmarks not only change x-y coordinates but also swap (the x-y coordinates for left eye landmarks are now right eye landmarks).</p>
<p>In all, our transforms to the image is as follows:
input -&gt; color jitter -&gt; grayscale -&gt; Random bounding box crop augmentations -&gt; random rotation and shift -&gt; resize to 128x128</p>
<p>
  <figure class="left" >
    <img src="/post/images/4-1.png"   />
    
      <figcaption class="center" >Example image with landmarks, pose, and vectors</figcaption>
    
  </figure>



  <figure class="left" >
    <img src="/post/images/4-2.png"   />
    
      <figcaption class="center" >Example image with landmarks (red and blue)</figcaption>
    
  </figure>

</p>
<p>The code for this dataloader can be found <a href="https://github.com/jinac/head_multitask/blob/main/data_util.py">here</a>.</p>
<h1 id="next-steps">Next steps</h1>
<p>Now, we have a face dataset containing all the labels we want to train on and code to load with augmentations. One thing to note is that we&rsquo;re using other models to get the labels, and we haven&rsquo;t gone through the data manually to clean it.</p>
<h1 id="links">Links</h1>
<ul>
<li><a href="https://medium.com/speechmatics/how-to-build-a-streaming-dataloader-with-pytorch-a66dd891d9dd">https://medium.com/speechmatics/how-to-build-a-streaming-dataloader-with-pytorch-a66dd891d9dd</a></li>
<li><a href="https://discuss.pytorch.org/t/how-to-create-a-dataloader-with-variable-size-input/8278/3">https://discuss.pytorch.org/t/how-to-create-a-dataloader-with-variable-size-input/8278/3</a></li>
<li><a href="https://pytorch.org/tutorials/beginner/data_loading_tutorial.html">https://pytorch.org/tutorials/beginner/data_loading_tutorial.html</a></li>
</ul>

    </div>
    
      
        <div class="pagination">
          <div class="pagination__title">
            <span class="pagination__title-h"
              >Read other posts</span
            >
            <hr />
          </div>
          <div class="pagination__buttons">
            
            
              <span class="button next">
                <a href="https://jinac.github.io/post/3/">
                  <span class="button__text">Old-School Computer Vision to Extract Baseball Stats</span>
                  <span class="button__icon">â†’</span>
                </a>
              </span>
            
          </div>
        </div>
      
    

    
      
        

      
    
  </div>

      </div>

      
        <footer class="footer">
  <div class="footer__inner">
    
      <a
  href="/"
  class="logo"
  style="text-decoration: none;"
>
  
    <span class="logo__mark"><svg xmlns="http://www.w3.org/2000/svg" class="greater-icon" viewBox="0 0 44 44">
  <path fill="none" d="M15 8l14.729 14.382L15 35.367" />
</svg>
</span>
    <span class="logo__text"
      >John Inacay</span
    >
    <span class="logo__cursor"></span>
  
</a>

      <div class="copyright">
        <span
          >Â© 2021 Powered by
          <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a></span
        >
        <span
          >Theme created by
          <a href="https://twitter.com/panr" target="_blank" rel="noopener">panr</a></span
        >
      </div>
    
  </div>
</footer>

<script src="https://jinac.github.io/assets/main.js"></script>
<script src="https://jinac.github.io/assets/prism.js"></script>


      
    </div>

    
  </body>
</html>
