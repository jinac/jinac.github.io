<!DOCTYPE html>
<html lang='en'><head>
  <title>Adapting the Superpoint Pytorch Model for OAK-1 - John Inacay</title>
  <link rel='canonical' href='https://jinac.github.io/post/2/' />
  <meta charset='utf-8'>
  <meta name='viewport' content='width=device-width, initial-scale=1' />
  <meta name='description' content='Tutorial for OpenVINO Chain Model Conversion' />
  <meta name='theme-color' content='#FD3519' />
  

  <meta name="generator" content="Hugo 0.74.3" />

  





<link rel="stylesheet" href="https://jinac.github.io/sass/style.min.8a1658d134a4b54730b66789206b2cf14c1b006a6de3f3fde6302f925b6e01f5.css" integrity="sha256-ihZY0TSktUcwtmeJIGss8UwbAGpt4/P95jAvkltuAfU=" media="screen">
<link rel="stylesheet" href="https://jinac.github.io/syntax.min.css" integrity="" media="screen">

  <meta property="og:title" content="Adapting the Superpoint Pytorch Model for OAK-1" />
<meta property="og:description" content="Tutorial for OpenVINO Chain Model Conversion" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://jinac.github.io/post/2/" />
<meta property="article:published_time" content="2020-10-04T21:32:29-07:00" />
<meta property="article:modified_time" content="2020-10-04T21:32:29-07:00" />

  <meta itemprop="name" content="Adapting the Superpoint Pytorch Model for OAK-1">
<meta itemprop="description" content="Tutorial for OpenVINO Chain Model Conversion">
<meta itemprop="datePublished" content="2020-10-04T21:32:29-07:00" />
<meta itemprop="dateModified" content="2020-10-04T21:32:29-07:00" />
<meta itemprop="wordCount" content="1140">



<meta itemprop="keywords" content="" />

</head>
<body>

  <header style="background-image:linear-gradient(
      rgba(0,0,0,0.4),rgba(0,0,0,0.4)
    ),url(&#39;https://jinac.github.io/images/Background.jpg&#39;)">

  <div class="intro">
    <div class="logo-container">
      <a href="/">
        <img src='https://jinac.github.io/images/Profile.jpeg' alt="Profile HOME" class="rounded-logo">
      </a>
    </div>
    <h2>John Inacay</h2>
    <h3>Computer Vision and Machine Learning Engineer, Composer and Multi-Instrumentalist</h3>
    <div class="menu">
      

        <p>
            <a href="/about/">
                About
            </a>
        </p>

        <p>
            <a href="/music/">
                Music
            </a>
        </p>

        <p>
            <a href="/resume/">
                Resume
            </a>
        </p>

        <p>
            <a href="/post/">
                Post
            </a>
        </p>

      
    </div>

  </div>

  <div class="socials">
      
  
    <a href="https://github.com/jinac" class="social-link" target="_blank" rel="noopener" ><div class="icon">
  <svg width="35px" height="35px" viewBox="0 0 115 115" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img">
  
  <path d="M102.679 0H12.32C5.52 0 0 5.519 0 12.321v90.358C0 109.48 5.519 115 12.321 115h90.358c6.802 0 12.321-5.519 12.321-12.321V12.32C115 5.52 109.481 0 102.679 0zM71.182 98.494c-2.156.385-2.952-.95-2.952-2.053 0-1.386.051-8.471.051-14.195 0-4.005-1.335-6.546-2.9-7.881C74.878 73.313 84.89 72.003 84.89 55.6c0-4.671-1.669-7.007-4.39-10.01.436-1.105 1.9-5.648-.436-11.552-3.568-1.104-11.731 4.595-11.731 4.595-3.389-.95-7.06-1.438-10.679-1.438-3.62 0-7.29.488-10.679 1.438 0 0-8.163-5.699-11.73-4.595-2.337 5.878-.899 10.422-.437 11.551-2.72 3.004-4.004 5.34-4.004 10.011 0 16.326 9.574 17.712 19.072 18.765-1.232 1.104-2.336 3.003-2.72 5.724-2.44 1.104-8.677 3.004-12.4-3.568-2.335-4.056-6.545-4.39-6.545-4.39-4.159-.05-.282 2.619-.282 2.619 2.772 1.283 4.723 6.212 4.723 6.212 2.49 7.624 14.4 5.057 14.4 5.057 0 3.568.052 9.37.052 10.422 0 1.104-.77 2.438-2.952 2.053C27.21 92.821 15.35 76.701 15.35 57.86c0-23.564 18.02-41.456 41.585-41.456s42.663 17.892 42.663 41.456c.026 18.842-11.474 34.988-28.416 40.635zM46 82.81c-.488.103-.95-.102-1.001-.436-.051-.385.282-.719.77-.822.488-.05.95.154 1.001.488.077.334-.257.668-.77.77zm-2.439-.23c0 .333-.385.615-.898.615-.565.052-.95-.23-.95-.616 0-.333.385-.616.899-.616.487-.051.95.231.95.616zm-3.516-.283c-.103.334-.616.488-1.053.334-.488-.103-.821-.488-.719-.822.103-.334.617-.488 1.053-.385.513.154.847.54.719.873zm-3.158-1.386c-.23.282-.718.23-1.104-.154-.385-.334-.487-.822-.23-1.053.23-.282.718-.23 1.103.154.334.334.462.847.231 1.053zm-2.336-2.336c-.23.154-.667 0-.95-.385-.282-.385-.282-.822 0-1.001.283-.231.72-.052.95.333.283.385.283.847 0 1.053zm-1.668-2.49c-.231.23-.616.103-.899-.154-.282-.334-.333-.719-.102-.899.23-.23.616-.102.898.154.282.334.334.72.103.899zm-1.72-1.9c-.103.231-.436.283-.719.103-.334-.154-.488-.436-.385-.667.103-.154.385-.231.719-.103.334.18.488.462.385.667z"/>
  
  </svg>
</div>
</a>
  

  
    <a href="https://www.linkedin.com/in/john-inacay-94356646/" class="social-link" target="_blank" rel="noopener" ><div class="icon">
  <svg width="35px" height="35px" viewBox="0 0 115 115" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img">
  
  <path d="M106.786 0H8.189C3.67 0 0 3.722 0 8.291v98.418C0 111.278 3.67 115 8.189 115h98.597c4.518 0 8.214-3.722 8.214-8.291V8.29C115 3.722 111.304 0 106.786 0zm-72.03 98.571H17.713V43.69h17.07V98.57h-.025zm-8.522-62.377c-5.467 0-9.882-4.44-9.882-9.883 0-5.442 4.415-9.882 9.882-9.882 5.442 0 9.883 4.44 9.883 9.882a9.87 9.87 0 0 1-9.883 9.883zm72.414 62.377H81.604V71.875c0-6.366-.129-14.555-8.856-14.555-8.882 0-10.242 6.931-10.242 14.093V98.57H45.46V43.69h16.352v7.495h.23c2.285-4.312 7.855-8.856 16.147-8.856 17.25 0 20.458 11.372 20.458 26.158V98.57z"/>
  
  </svg>
</div>
</a>
  

  
    <a href="https://medium.com/@jdevinacay" class="social-link" target="_blank" rel="noopener" ><div class="icon">
  <svg width="35px" height="35px" viewBox="0 0 115 115" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img">
  
  <path d="M0 0v115h115V0H0zm95.542 27.235l-6.16 5.905a1.81 1.81 0 0 0-.693 1.72v43.458c-.103.667.154 1.335.693 1.72l6.032 5.904v1.31h-30.29v-1.259l6.238-6.058c.616-.616.616-.795.616-1.72V43.074L54.625 87.123h-2.336l-20.202-44.05v29.52c-.18 1.233.257 2.49 1.13 3.39l8.111 9.83v1.31H18.277v-1.31l8.111-9.83a3.93 3.93 0 0 0 1.053-3.39v-34.14a2.93 2.93 0 0 0-.976-2.516l-7.213-8.702v-1.309h22.41l17.301 37.991 15.222-37.965h21.357v1.283z"/>
  
  </svg>
</div>
</a>
  

  
    <a href="https://twitter.com/jincfunk" class="social-link" target="_blank" rel="noopener" ><div class="icon">
  <svg width="35px" height="35px" viewBox="0 0 115 115" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img">
  
  <path d="M102.679 0H12.32C5.52 0 0 5.519 0 12.321v90.358C0 109.48 5.519 115 12.321 115h90.358c6.802 0 12.321-5.519 12.321-12.321V12.32C115 5.52 109.481 0 102.679 0zM90.126 40.763c.051.72.051 1.464.051 2.182 0 22.256-16.942 47.9-47.9 47.9-9.548 0-18.404-2.772-25.848-7.547 1.36.154 2.67.205 4.055.205 7.881 0 15.12-2.67 20.895-7.187-7.392-.154-13.604-5.006-15.735-11.68 2.593.385 4.929.385 7.598-.308a16.837 16.837 0 0 1-13.476-16.531v-.205a16.824 16.824 0 0 0 7.598 2.13 16.8 16.8 0 0 1-7.496-14.016c0-3.131.822-6.006 2.285-8.496a47.803 47.803 0 0 0 34.705 17.61c-2.387-11.424 6.161-20.69 16.429-20.69 4.851 0 9.215 2.027 12.296 5.313a32.99 32.99 0 0 0 10.678-4.056 16.792 16.792 0 0 1-7.393 9.267c3.389-.36 6.674-1.31 9.703-2.618a35.437 35.437 0 0 1-8.445 8.727z"/>
  
  </svg>
</div>
</a>
  

  </div>

</header>

  <div class="content-wrapper">
    
      <div class="breadcrumb">
  





<span >
  <a href="https://jinac.github.io/">HOME</a>
   / 
</span>


<span >
  <a href="https://jinac.github.io/post/"></a>
   / 
</span>


<span  class="active">
  <a href="https://jinac.github.io/post/2/">Adapting the Superpoint Pytorch Model for OAK-1</a>
  
</span>

</div>

    
    <main id="content" class="post">

<h1>Adapting the Superpoint Pytorch Model for OAK-1</h1>
<div class="reading-time">
  <div class="icon">
  <svg width="18px" height="18px" viewBox="0 0 115 115" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img">
  
  <path d="M57.5 11C29.05 11 6 34.05 6 62.5S29.05 114 57.5 114 109 90.95 109 62.5 85.95 11 57.5 11zm0 93.032c-22.947 0-41.532-18.585-41.532-41.532 0-22.947 18.585-41.532 41.532-41.532 22.947 0 41.532 18.585 41.532 41.532 0 22.947-18.585 41.532-41.532 41.532zm12.833-21.68L52.703 69.54a2.508 2.508 0 0 1-1.018-2.015V33.427a2.5 2.5 0 0 1 2.492-2.492h6.646a2.5 2.5 0 0 1 2.492 2.492v29.426l13.871 10.092c1.122.81 1.35 2.368.54 3.49l-3.904 5.377a2.51 2.51 0 0 1-3.489.54z"/>
  
  </svg>
</div>

  <span>6 minutes</span>
</div>

<div class="published-date">
  <div class="icon">
  <svg width="18px" height="18px" viewBox="0 0 115 115" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img">
  
  <path d="M77.577 51.23a1.807 1.807 0 0 0-2.2.342l-27.562 27.79a1.807 1.807 0 0 1-2.2.342l-14.008-9.702a1.807 1.807 0 0 0-2.2.342l-1.952 1.968c-.287.22-.456.568-.455.936.001.37.172.716.46.934L45.637 86.77a1.807 1.807 0 0 0 2.2-.342l31.709-31.97c.287-.22.456-.567.455-.936a1.175 1.175 0 0 0-.46-.933l-1.963-1.36z"/><path d="M97.304 20H80.512c-.041.34-.063.683-.064 1.026a5.986 5.986 0 0 0 1.256 4.1c.054.003.103.02.157.025a4.881 4.881 0 0 1 1.865-.025c3.05.562 4.984 3.907 4.32 7.47-.666 3.563-3.678 5.996-6.728 5.433a4.932 4.932 0 0 1-2.437-1.258c-6.018-1.378-10.445-7.795-10.445-15.745 0-.347.023-.685.04-1.026H34.579c-.041.34-.063.683-.064 1.026a5.986 5.986 0 0 0 1.256 4.1c.054.003.103.02.157.025a4.881 4.881 0 0 1 1.865-.025c3.05.562 4.984 3.907 4.32 7.47-.666 3.563-3.678 5.996-6.728 5.433a4.932 4.932 0 0 1-2.437-1.258c-6.018-1.378-10.445-7.795-10.445-15.745 0-.22.019-.434.025-.652a9.788 9.788 0 0 0-5.697 4.471 9.683 9.683 0 0 0-2.65 4.764L1.158 92.871c-.965 4.689 2.6 8.503 7.948 8.503h6.334v2.673c-.077 5.41 4.263 9.861 9.705 9.953h72.16c5.438-.095 9.774-4.546 9.694-9.953V29.953c.08-5.407-4.256-9.858-9.695-9.953zM10.078 96.653c-2.378 0-3.964-1.697-3.535-3.782L16.637 43.84h80.787L87.331 92.871a5.254 5.254 0 0 1-5.091 3.782H10.078zm91.535 7.394c.036 2.403-1.891 4.382-4.308 4.424h-72.16c-2.42-.04-4.352-2.018-4.32-4.424v-2.673h60.443c5.348 0 10.484-3.814 11.449-8.503l8.897-43.215v54.391z"/><path d="M34.814 33c1.243 0 2.251-1.057 2.251-2.36 0-1.305-1.008-2.362-2.25-2.362-2.04 0-4.313-3.194-4.313-7.778s2.272-7.778 4.312-7.778c1.227 0 2.536 1.163 3.386 3.084H43C41.716 11.19 38.578 8 34.814 8 29.871 8 26 13.49 26 20.5c0 7.009 3.871 12.5 8.814 12.5z"/>
  
  </svg>
</div>

  <span>October 4, 2020</span>
</div>

<h1 id="introduction">Introduction</h1>
<p>Deep learning model deployment doesn&rsquo;t end with the training of a model. Especially with edge devices and the variety of processors, there can be many steps to get a network running on an embedded device. With the <a href="https://opencv.org/introducing-oak-spatial-ai-powered-by-opencv/">OpenCV AI Kit</a>, I have camera modules with a Myriad X chip on the same board. It is a module well-suited for deep learning inference with what should be minimal bottleneck of image data transfer.</p>
<p><img src="/post/images/2-1.jpg" alt="My OAK-1">
I now have an OAK-1 (well, an earlier run MegaAI module), and I want to familiarize myself with the device. While the <a href="https://docs.luxonis.com/">docs</a> from the designers of the OAK-1 Luxonis show they have a model zoo, I am more interested in how to use your own networks with the device. Considering its design, the OAK devices are most suited for inference directly on image with minimal (if any) pre-processing. Tasks like keypoint and descriptor extraction (like <a href="https://github.com/magicleap/SuperPointPretrainedNetwork">Superpoint</a>) and object detection (like <a href="https://pjreddie.com/darknet/yolo/">YOLO</a>) are good candidates for tasks to be taken up on this module. Since YOLO is already supported in the model zoo, I&rsquo;ve gone through the steps of converting the pretrained pytorch Superpoint network to the Movidius Blob format for the Myriad X. Because not all architectures of networks are the same or even originate from the same framework, this does not illustrate the process of adapting all models. I will discuss some considerations you may need to make, but YMMV!
<img src="/post/images/2-2.jpg" alt="Also my OAK-1"></p>
<h1 id="conversion-process">Conversion Process</h1>
<p>My code for performing all these steps can be found in my <a href="https://github.com/jinac/superpoint_infer_engine">github repo</a>, but I&rsquo;ll put snippets here.</p>
<h2 id="prerequisites-and-considerations">Prerequisites and Considerations</h2>
<p>Since the Myriad X is the target, you first need to install the Intel <a href="https://docs.openvinotoolkit.org/latest/index.html">OpenVINO Toolkit</a>. At the time of writing, you should use version 2020.1 or 2020.2, especially if you&rsquo;re planning to use the DepthAI python library to use the OAK device.
Assuming one has already has a pretrained model or will be training a model, there some things to be aware of that will affect the amount of work in later steps. You should be aware of the following:</p>
<h3 id="1-check-on-layer-compatibility-for-each-intermediate-step">1. Check on layer compatibility for each intermediate step.</h3>
<p>Layers may be incompatible or not have an implementatino between different conversion steps. In this work, I removed some incompatible layers in the ONNX conversion step and re-implemented them in numpy. When converting from ONNX to IR, you may face more layers that are not implemented. When you can&rsquo;t run the full model on the OAK device due to incompatibile layers, you could:</p>
<ul>
<li>Implement the layers for OpenVINO tools</li>
<li>Remove the layers from the Myriad X computation and run further computation on the host of OAK device</li>
<li>Train a model that doesn&rsquo;t have such incompatibilities</li>
</ul>
<h3 id="2-adjust-your-model-to-meet-your-runtime-needs">2. Adjust your model to meet your runtime needs.</h3>
<p>The Myriad X is for inference, but it&rsquo;s still a more limited resource than your desktop or server GPU. Figure out what adjustments you can make (i.e. batch size, input resolution, etc.) and tune these parameters when training or converting models to sufficiently run on the Myriad X</p>
<h2 id="convert-pytorch-model-to-onnx">Convert Pytorch Model to ONNX</h2>
<p>The first step is to convert the pytorch model to ONNX as an intermediate representation. Intel&rsquo;s IR models do not have a direct conversion from pytorch, but they do have conversion tooling from ONNX. As mentioned earlier, some layers or operations may not be supported in the conversion to ONNX. While Superpoint is mostly a CNN, the l2-norm is not implemented in ONNX. It&rsquo;s used as a final operation on the coarse descriptor output, so I just re-implement it and absorb it into the post-processing of the model output that happens in superpoint_frontend.py.</p>
<p>The pytorch library already supports conversion to onnx. Part of the <a href="https://github.com/jinac/superpoint_infer_engine/blob/master/convert_onnx.py">conversion script</a> is reproduced below to show the relevant steps. Of note, the opset_version should be 10 for the supported OpenVINO versions at the time of writing.</p>
<div class="highlight"><pre class="chroma"><code class="language-text" data-lang="text">import numpy as np
import torch


import network
from superpoint_frontend import reduce_l2

weights_path = &#39;weights/superpoint_v1.pth&#39;
h = 100  # height of input image in pixels
w = 100  # width of input image in pixels

# Create model object.
pt_model = network.SuperPointNet()
pytorch_total_params = sum(p.numel() for p in pt_model.parameters())
print(&#39;Total number of params: &#39;, pytorch_total_params)

# Initialize model with the pretrained weights
map_location = lambda storage, loc: storage
if torch.cuda.is_available():
    map_location = None
pt_model.load_state_dict(torch.load(weights_path, map_location=map_location))
pt_model.eval()

# Create random input and run on model for onnx trace.
x = torch.randn(batch_size, 1, h, w, requires_grad=True)
torch_out = pt_model(x)

# Export the model
onnx_filename = os.path.join(output_dir, &#34;superpoint_{}x{}.onnx&#34;.format(h, w))
torch.onnx.export(pt_model,               # model being run
                  x,                         # model input (or a tuple for multiple inputs)
                  onnx_filename,   # where to save the model (can be a file or file-like object)
                  export_params=True,        # store the trained parameter weights inside the model file
                  opset_version=10,          # the ONNX version to export the model to
                  do_constant_folding=True,  # whether to execute constant folding for optimization
                  input_names = [&#39;input&#39;],   # the model&#39;s input names
                  output_names = [&#39;semi&#39;, &#39;desc&#39;], # the model&#39;s output names
                  )
# Output ONNX model is saved to &#34;output/superpoint_100x100.onnx&#34;, expecting an input image of [1,1,100,100]
</code></pre></div><h2 id="convert-onnx-model-to-intel-intermediate-representation-ir">Convert ONNX Model to Intel Intermediate Representation (IR)</h2>
<p>Now that I have an ONNX model, I need to convert it to Intel&rsquo;s IR. This IR can be used as is if targeting other devices and will serve as the input to a final step of converting to the Movidius Blob to be run on the Myriad X. I will use the OpenVINO model optimizer tools. Assuming OpenVINO is installed in /opt/intel/openvino, the following script would need to be run:</p>
<div class="highlight"><pre class="chroma"><code class="language-text" data-lang="text">python3 /opt/intel/openvino/deployment_tools/model_optimizer/mo.py --input_shape [1,1,100,100] --input_model output/superpoint_100x100.onnx --output_dir output --data_type FP16
# FP16 data type is due to the Myriad X expecting weights and layers to be fp16 ops
# Output model file (superpoint_100x100.xml) and weights file (superpoint100x100.bin) to directory &#34;output/&#34;
</code></pre></div><h2 id="generate-intel-ir-to-movidius-blob">Generate Intel IR to Movidius Blob</h2>
<p>Finally, I can now get our model in the Myriad X format.</p>
<div class="highlight"><pre class="chroma"><code class="language-text" data-lang="text">/opt/intel/openvino/deployment_tools/inference_engine/lib/intel64/myriad_compile -m output/superpoint_100x100.xml -o output/superpoint_100x100.blob -ip U8 -VPU_MYRIAD_PLATFORM VPU_MYRIAD_2480 -VPU_NUMBER_OF_SHAVES 4 -VPU_NUMBER_OF_CMX_SLICES 4
# Output model to &#34;output/superpoint_100x100.blob&#34;
</code></pre></div><p>You also will need json file called &ldquo;superpoint_100x100.json&rdquo; for the OAK device DepthAI api to use the model. This is automated in <a href="https://github.com/jinac/superpoint_infer_engine/blob/master/convert_onnx.py">onnx conversion script</a>. This json file defines the output tensors of the model for parsing (but multiple output tensors may require more involved processing, see <a href="https://github.com/jinac/superpoint_infer_engine/blob/master/run_oak.py">run_oak.py</a> for an example).</p>
<p>superpoint_100x100.json</p>
<div class="highlight"><pre class="chroma"><code class="language-text" data-lang="text">{
    &#34;tensors&#34;:
    [
        {       
            &#34;output_tensor_name&#34;: &#34;semi&#34;,
            &#34;output_dimensions&#34;: [1, 65, 12, 12]
            &#34;output_entry_iteration_index&#34;: 0,
            &#34;output_properties_dimensions&#34;: [0],
            &#34;property_key_mapping&#34;: [],
            &#34;output_properties_type&#34;: &#34;f16&#34;
        },
        {       
            &#34;output_tensor_name&#34;: &#34;desc&#34;,
            &#34;output_dimensions&#34;: [1, 256, 12, 12],
            &#34;output_entry_iteration_index&#34;: 0,
            &#34;output_properties_dimensions&#34;: [0],
            &#34;property_key_mapping&#34;: [],
            &#34;output_properties_type&#34;: &#34;f16&#34;
        },          
    ]
}
</code></pre></div><h1 id="conclusion">Conclusion</h1>
<p>I have taken the superpoint pretrained model and adapted it to function on an OAK device. I share some obstacles that may be common in the chain conversion process and provide some suggestions to tackle them. I hope that this tutorial provides a useful blueprint for deploying your own models for the OAK device or other Intel device targets.</p>


    </main>
  </div>
  <footer>
    <div class="footer-wrapper">
      <p>Made with ❤️ &mdash; Powered by <a href="https://gohugo.io/" target="_blank" rel="external">Hugo</a> and the <a href="https://github.com/bjacquemet/personal-web" target='_blank' rel="external">Personal Web</a> theme. Icons come from the great <a href="https://fontawesome.com/license" target="_blank" rel="external">Font Awesome</a> library</p>
      <p>© John Inacay</p>
    </div>
  </footer>
  <link href="https://fonts.googleapis.com/css?family=Montserrat:500,600|Raleway:400,400i,600" rel="stylesheet">
  
</body>
</html>
