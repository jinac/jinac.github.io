<!DOCTYPE html>
<html lang="en">
  <head>
    
      <title>
        Adapting the Superpoint Pytorch Model for OAK-1 ::
        John Inacay — Computer Vision and Machine Learning Engineer // Composer and Multi-Instrumentalist
      </title>
    
    <meta http-equiv="content-type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<meta
  name="description"
  content="Introduction Deep learning model deployment doesn&amp;rsquo;t end with the training of a model. Especially with edge devices and the variety of processors, there can be many steps to get a network running on an embedded device. With the OpenCV AI Kit, I have camera modules with a Myriad X chip on the same board. It is a module well-suited for deep learning inference with what should be minimal bottleneck of image data transfer."
/>
<meta
  name="keywords"
  content=""
/>
<meta name="robots" content="noodp" />
<link rel="canonical" href="https://jinac.github.io/post/2/" />





<link rel="stylesheet" href="https://jinac.github.io/assets/style.css" />

<link rel="stylesheet" href="https://jinac.github.io/style.css" />


<link
  rel="apple-touch-icon-precomposed"
  sizes="144x144"
  href="https://jinac.github.io/img/apple-touch-icon-144-precomposed.png"
/>
<link rel="shortcut icon" href="https://jinac.github.io/img/favicon.png" />


<link href="https://jinac.github.io/assets/fonts/Inter-Italic.woff2" rel="preload" type="font/woff2" as="font" crossorigin="">
<link href="https://jinac.github.io/assets/fonts/Inter-Regular.woff2" rel="preload" type="font/woff2" as="font" crossorigin="">
<link href="https://jinac.github.io/assets/fonts/Inter-Medium.woff2" rel="preload" type="font/woff2" as="font" crossorigin="">
<link href="https://jinac.github.io/assets/fonts/Inter-MediumItalic.woff2" rel="preload" type="font/woff2" as="font" crossorigin="">
<link href="https://jinac.github.io/assets/fonts/Inter-Bold.woff2" rel="preload" type="font/woff2" as="font" crossorigin="">
<link href="https://jinac.github.io/assets/fonts/Inter-BoldItalic.woff2" rel="preload" type="font/woff2" as="font" crossorigin="">


<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Adapting the Superpoint Pytorch Model for OAK-1"/>
<meta name="twitter:description" content="Tutorial for OpenVINO Chain Model Conversion"/>



<meta property="og:title" content="Adapting the Superpoint Pytorch Model for OAK-1" />
<meta property="og:description" content="Tutorial for OpenVINO Chain Model Conversion" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://jinac.github.io/post/2/" />
<meta property="article:published_time" content="2020-10-04T21:32:29-07:00" />
<meta property="article:modified_time" content="2020-10-04T21:32:29-07:00" /><meta property="og:site_name" content="John Inacay" />






  </head>
  <body class="dark-theme">
    <div class="container">
      <header class="header">
  <span class="header__inner">
    <a
  href="/"
  class="logo"
  style="text-decoration: none;"
>
  
    <span class="logo__mark"><svg xmlns="http://www.w3.org/2000/svg" class="greater-icon" viewBox="0 0 44 44">
  <path fill="none" d="M15 8l14.729 14.382L15 35.367" />
</svg>
</span>
    <span class="logo__text"
      >John Inacay</span
    >
    <span class="logo__cursor"></span>
  
</a>

    <span class="header__right">
      
        <nav class="menu">
  <ul class="menu__inner menu__inner--desktop">
    
      
        
          <li><a href="/about/">About</a></li>
        
      
        
          <li><a href="/post/">Blog</a></li>
        
      
        
          <li><a href="/music/">Music</a></li>
        
      
        
          <li><a href="/resume/">Resume</a></li>
        
      
      
      
  </ul>

  <ul class="menu__inner menu__inner--mobile">
    
      
        <li><a href="/about/">About</a></li>
      
    
      
        <li><a href="/post/">Blog</a></li>
      
    
      
        <li><a href="/music/">Music</a></li>
      
    
      
        <li><a href="/resume/">Resume</a></li>
      
    
  </ul>
</nav>

        <span class="menu-trigger">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
            <path d="M0 0h24v24H0z" fill="none" />
            <path d="M3 18h18v-2H3v2zm0-5h18v-2H3v2zm0-7v2h18V6H3z" />
          </svg>
        </span>
      
      <span class="theme-toggle">
        <svg
  class="theme-toggler"
  width="24"
  height="24"
  viewBox="0 0 48 48"
  fill="none"
  xmlns="http://www.w3.org/2000/svg"
>
  <path
    d="M22 41C32.4934 41 41 32.4934 41 22C41 11.5066 32.4934 3 22
  3C11.5066 3 3 11.5066 3 22C3 32.4934 11.5066 41 22 41ZM7 22C7
  13.7157 13.7157 7 22 7V37C13.7157 37 7 30.2843 7 22Z"
  />
</svg>

      </span>
    </span>
  </span>
</header>


      <div class="content">
        
  
  

  <div class="post">
    <h1 class="post-title">Adapting the Superpoint Pytorch Model for OAK-1</h1>
    <div class="post-meta">
      
        <span class="post-date">
          2020-10-04
        </span>

        
          
        
      

      


      
    </div>

    

    

    <div class="post-content">
      
        <h2>Table of Contents</h2>
        <aside class="table-of-contents"><nav id="TableOfContents">
  <ul>
    <li><a href="#prerequisites-and-considerations">Prerequisites and Considerations</a>
      <ul>
        <li><a href="#1-check-on-layer-compatibility-for-each-intermediate-step">1. Check on layer compatibility for each intermediate step.</a></li>
        <li><a href="#2-adjust-your-model-to-meet-your-runtime-needs">2. Adjust your model to meet your runtime needs.</a></li>
      </ul>
    </li>
    <li><a href="#convert-pytorch-model-to-onnx">Convert Pytorch Model to ONNX</a></li>
    <li><a href="#convert-onnx-model-to-intel-intermediate-representation-ir">Convert ONNX Model to Intel Intermediate Representation (IR)</a></li>
    <li><a href="#generate-intel-ir-to-movidius-blob">Generate Intel IR to Movidius Blob</a></li>
  </ul>
</nav></aside>
      
      <h1 id="introduction">Introduction</h1>
<p>Deep learning model deployment doesn&rsquo;t end with the training of a model. Especially with edge devices and the variety of processors, there can be many steps to get a network running on an embedded device. With the <a href="https://opencv.org/introducing-oak-spatial-ai-powered-by-opencv/">OpenCV AI Kit</a>, I have camera modules with a Myriad X chip on the same board. It is a module well-suited for deep learning inference with what should be minimal bottleneck of image data transfer.</p>

  <figure class="left" >
    <img src="/post/images/2-1.jpg"   />
    
      <figcaption class="center" >My OAK-1</figcaption>
    
  </figure>


<p>I now have an OAK-1 (well, an earlier run MegaAI module), and I want to familiarize myself with the device. While the <a href="https://docs.luxonis.com/">docs</a> from the designers of the OAK-1 Luxonis show they have a model zoo, I am more interested in how to use your own networks with the device. Considering its design, the OAK devices are most suited for inference directly on image with minimal (if any) pre-processing. Tasks like keypoint and descriptor extraction (like <a href="https://github.com/magicleap/SuperPointPretrainedNetwork">Superpoint</a>) and object detection (like <a href="https://pjreddie.com/darknet/yolo/">YOLO</a>) are good candidates for tasks to be taken up on this module. Since YOLO is already supported in the model zoo, I&rsquo;ve gone through the steps of converting the pretrained pytorch Superpoint network to the Movidius Blob format for the Myriad X. Because not all architectures of networks are the same or even originate from the same framework, this does not illustrate the process of adapting all models. I will discuss some considerations you may need to make, but YMMV!</p>

  <figure class="left" >
    <img src="/post/images/2-2.jpg"   />
    
      <figcaption class="center" >Also my OAK-1</figcaption>
    
  </figure>


<h1 id="conversion-process">Conversion Process</h1>
<p>My code for performing all these steps can be found in my <a href="https://github.com/jinac/superpoint_infer_engine">github repo</a>, but I&rsquo;ll put snippets here.</p>
<h2 id="prerequisites-and-considerations">Prerequisites and Considerations</h2>
<p>Since the Myriad X is the target, you first need to install the Intel <a href="https://docs.openvinotoolkit.org/latest/index.html">OpenVINO Toolkit</a>. At the time of writing, you should use version 2020.1 or 2020.2, especially if you&rsquo;re planning to use the DepthAI python library to use the OAK device.
Assuming one has already has a pretrained model or will be training a model, there some things to be aware of that will affect the amount of work in later steps. You should be aware of the following:</p>
<h3 id="1-check-on-layer-compatibility-for-each-intermediate-step">1. Check on layer compatibility for each intermediate step.</h3>
<p>Layers may be incompatible or not have an implementatino between different conversion steps. In this work, I removed some incompatible layers in the ONNX conversion step and re-implemented them in numpy. When converting from ONNX to IR, you may face more layers that are not implemented. When you can&rsquo;t run the full model on the OAK device due to incompatibile layers, you could:</p>
<ul>
<li>Implement the layers for OpenVINO tools</li>
<li>Remove the layers from the Myriad X computation and run further computation on the host of OAK device</li>
<li>Train a model that doesn&rsquo;t have such incompatibilities</li>
</ul>
<h3 id="2-adjust-your-model-to-meet-your-runtime-needs">2. Adjust your model to meet your runtime needs.</h3>
<p>The Myriad X is for inference, but it&rsquo;s still a more limited resource than your desktop or server GPU. Figure out what adjustments you can make (i.e. batch size, input resolution, etc.) and tune these parameters when training or converting models to sufficiently run on the Myriad X</p>
<h2 id="convert-pytorch-model-to-onnx">Convert Pytorch Model to ONNX</h2>
<p>The first step is to convert the pytorch model to ONNX as an intermediate representation. Intel&rsquo;s IR models do not have a direct conversion from pytorch, but they do have conversion tooling from ONNX. As mentioned earlier, some layers or operations may not be supported in the conversion to ONNX. While Superpoint is mostly a CNN, the l2-norm is not implemented in ONNX. It&rsquo;s used as a final operation on the coarse descriptor output, so I just re-implement it and absorb it into the post-processing of the model output that happens in superpoint_frontend.py.</p>
<p>The pytorch library already supports conversion to onnx. Part of the <a href="https://github.com/jinac/superpoint_infer_engine/blob/master/convert_onnx.py">conversion script</a> is reproduced below to show the relevant steps. Of note, the opset_version should be 10 for the supported OpenVINO versions at the time of writing.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> numpy <span style="color:#f92672">as</span> np
<span style="color:#f92672">import</span> torch


<span style="color:#f92672">import</span> network
<span style="color:#f92672">from</span> superpoint_frontend <span style="color:#f92672">import</span> reduce_l2

weights_path <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;weights/superpoint_v1.pth&#39;</span>
h <span style="color:#f92672">=</span> <span style="color:#ae81ff">100</span>  <span style="color:#75715e"># height of input image in pixels</span>
w <span style="color:#f92672">=</span> <span style="color:#ae81ff">100</span>  <span style="color:#75715e"># width of input image in pixels</span>

<span style="color:#75715e"># Create model object.</span>
pt_model <span style="color:#f92672">=</span> network<span style="color:#f92672">.</span>SuperPointNet()
pytorch_total_params <span style="color:#f92672">=</span> sum(p<span style="color:#f92672">.</span>numel() <span style="color:#66d9ef">for</span> p <span style="color:#f92672">in</span> pt_model<span style="color:#f92672">.</span>parameters())
<span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#39;Total number of params: &#39;</span>, pytorch_total_params)

<span style="color:#75715e"># Initialize model with the pretrained weights</span>
map_location <span style="color:#f92672">=</span> <span style="color:#66d9ef">lambda</span> storage, loc: storage
<span style="color:#66d9ef">if</span> torch<span style="color:#f92672">.</span>cuda<span style="color:#f92672">.</span>is_available():
  map_location <span style="color:#f92672">=</span> None
pt_model<span style="color:#f92672">.</span>load_state_dict(torch<span style="color:#f92672">.</span>load(weights_path, map_location<span style="color:#f92672">=</span>map_location))
pt_model<span style="color:#f92672">.</span>eval()

<span style="color:#75715e"># Create random input and run on model for onnx trace.</span>
x <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>randn(batch_size, <span style="color:#ae81ff">1</span>, h, w, requires_grad<span style="color:#f92672">=</span>True)
torch_out <span style="color:#f92672">=</span> pt_model(x)

<span style="color:#75715e"># Export the model</span>
onnx_filename <span style="color:#f92672">=</span> os<span style="color:#f92672">.</span>path<span style="color:#f92672">.</span>join(output_dir, <span style="color:#e6db74">&#34;superpoint_{}x{}.onnx&#34;</span><span style="color:#f92672">.</span>format(h, w))
torch<span style="color:#f92672">.</span>onnx<span style="color:#f92672">.</span>export(
  pt_model,               <span style="color:#75715e"># model being run</span>
  x,                         <span style="color:#75715e"># model input (or a tuple for multiple inputs)</span>
  onnx_filename,   <span style="color:#75715e"># where to save the model (can be a file or file-like object)</span>
  export_params<span style="color:#f92672">=</span>True,        <span style="color:#75715e"># store the trained parameter weights inside the model file</span>
  opset_version<span style="color:#f92672">=</span><span style="color:#ae81ff">10</span>,          <span style="color:#75715e"># the ONNX version to export the model to</span>
  do_constant_folding<span style="color:#f92672">=</span>True,  <span style="color:#75715e"># whether to execute constant folding for optimization</span>
  input_names <span style="color:#f92672">=</span> [<span style="color:#e6db74">&#39;input&#39;</span>],   <span style="color:#75715e"># the model&#39;s input names</span>
  output_names <span style="color:#f92672">=</span> [<span style="color:#e6db74">&#39;semi&#39;</span>, <span style="color:#e6db74">&#39;desc&#39;</span>], <span style="color:#75715e"># the model&#39;s output names</span>
  )
<span style="color:#75715e"># Output ONNX model is saved to &#34;output/superpoint_100x100.onnx&#34;, expecting an input image of [1,1,100,100]</span>
</code></pre></div><h2 id="convert-onnx-model-to-intel-intermediate-representation-ir">Convert ONNX Model to Intel Intermediate Representation (IR)</h2>
<p>Now that I have an ONNX model, I need to convert it to Intel&rsquo;s IR. This IR can be used as is if targeting other devices and will serve as the input to a final step of converting to the Movidius Blob to be run on the Myriad X. I will use the OpenVINO model optimizer tools. Assuming OpenVINO is installed in /opt/intel/openvino, the following script would need to be run:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sh" data-lang="sh">python3 /opt/intel/openvino/deployment_tools/model_optimizer/mo.py --input_shape <span style="color:#f92672">[</span>1,1,100,100<span style="color:#f92672">]</span> --input_model output/superpoint_100x100.onnx --output_dir output --data_type FP16
<span style="color:#75715e"># FP16 data type is due to the Myriad X expecting weights and layers to be fp16 ops</span>
<span style="color:#75715e"># Output model file (superpoint_100x100.xml) and weights file (superpoint100x100.bin) to directory &#34;output/&#34;</span>
</code></pre></div><p>We have <a href="https://github.com/jinac/superpoint_infer_engine/blob/master/run_openvino.py">an example script</a> to use this model. We should expect an output like this:</p>

  <figure class="left" >
    <img src="/post/images/2-3.gif"   />
    
      <figcaption class="center" >run_openvino.py output</figcaption>
    
  </figure>


<h2 id="generate-intel-ir-to-movidius-blob">Generate Intel IR to Movidius Blob</h2>
<p>Finally, I can now get our model in the Myriad X format.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sh" data-lang="sh">/opt/intel/openvino/deployment_tools/inference_engine/lib/intel64/myriad_compile -m output/superpoint_100x100.xml -o output/superpoint_100x100.blob -ip U8 -VPU_MYRIAD_PLATFORM VPU_MYRIAD_2480 -VPU_NUMBER_OF_SHAVES <span style="color:#ae81ff">4</span> -VPU_NUMBER_OF_CMX_SLICES <span style="color:#ae81ff">4</span>
<span style="color:#75715e"># Output model to &#34;output/superpoint_100x100.blob&#34;</span>
</code></pre></div><p>You also will need json file called &ldquo;superpoint_100x100.json&rdquo; for the OAK device DepthAI api to use the model. This is automated in <a href="https://github.com/jinac/superpoint_infer_engine/blob/master/convert_onnx.py">onnx conversion script</a>. This json file defines the output tensors of the model for parsing (but multiple output tensors may require more involved processing, see <a href="https://github.com/jinac/superpoint_infer_engine/blob/master/run_oak.py">run_oak.py</a> for an example).</p>
<p>superpoint_100x100.json</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json">{
  <span style="color:#f92672">&#34;tensors&#34;</span>:
  [
    {       
      <span style="color:#f92672">&#34;output_tensor_name&#34;</span>: <span style="color:#e6db74">&#34;semi&#34;</span>,
      <span style="color:#f92672">&#34;output_dimensions&#34;</span>: [<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">65</span>, <span style="color:#ae81ff">12</span>, <span style="color:#ae81ff">12</span>],
      <span style="color:#f92672">&#34;output_entry_iteration_index&#34;</span>: <span style="color:#ae81ff">0</span>,
      <span style="color:#f92672">&#34;output_properties_dimensions&#34;</span>: [<span style="color:#ae81ff">0</span>],
      <span style="color:#f92672">&#34;property_key_mapping&#34;</span>: [],
      <span style="color:#f92672">&#34;output_properties_type&#34;</span>: <span style="color:#e6db74">&#34;f16&#34;</span>
    },
    {       
      <span style="color:#f92672">&#34;output_tensor_name&#34;</span>: <span style="color:#e6db74">&#34;desc&#34;</span>,
      <span style="color:#f92672">&#34;output_dimensions&#34;</span>: [<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">256</span>, <span style="color:#ae81ff">12</span>, <span style="color:#ae81ff">12</span>],
      <span style="color:#f92672">&#34;output_entry_iteration_index&#34;</span>: <span style="color:#ae81ff">0</span>,
      <span style="color:#f92672">&#34;output_properties_dimensions&#34;</span>: [<span style="color:#ae81ff">0</span>],
      <span style="color:#f92672">&#34;property_key_mapping&#34;</span>: [],
      <span style="color:#f92672">&#34;output_properties_type&#34;</span>: <span style="color:#e6db74">&#34;f16&#34;</span>
    },          
  ]
}
</code></pre></div><p>I don&rsquo;t have an example output here because I couldn&rsquo;t get similar performance that I got out of the IR model. Some things to explore for this particular case might be looking at how OAK-1 does frame pre-processing as I subsample from the full frame and the OAK-1 seems like it&rsquo;s center cropping instead. Another aspect is that I notice some speed issues, and a smaller trained model would improve that aspect. (Perhaps trying using a smaller CNN and <a href="https://arxiv.org/abs/1503.02531">distilling</a>)</p>
<h1 id="conclusion">Conclusion</h1>
<p>I have taken the superpoint pretrained model and adapted it to function on an OAK device. I share some obstacles that may be common in the chain conversion process and provide some suggestions to tackle them. I hope that this tutorial provides a useful blueprint for deploying your own models for the OAK device or other Intel device targets.</p>

    </div>
    
      
        <div class="pagination">
          <div class="pagination__title">
            <span class="pagination__title-h"
              >Read other posts</span
            >
            <hr />
          </div>
          <div class="pagination__buttons">
            
              <span class="button previous">
                <a href="https://jinac.github.io/post/4/">
                  <span class="button__icon">←</span>
                  <span class="button__text">Creating a Face Multitask Dataset</span>
                </a>
              </span>
            
            
              <span class="button next">
                <a href="https://jinac.github.io/post/1/">
                  <span class="button__text">What are Transformers?</span>
                  <span class="button__icon">→</span>
                </a>
              </span>
            
          </div>
        </div>
      
    

    
      
        

      
    
  </div>

      </div>

      
        <footer class="footer">
  <div class="footer__inner">
    
      <a
  href="/"
  class="logo"
  style="text-decoration: none;"
>
  
    <span class="logo__mark"><svg xmlns="http://www.w3.org/2000/svg" class="greater-icon" viewBox="0 0 44 44">
  <path fill="none" d="M15 8l14.729 14.382L15 35.367" />
</svg>
</span>
    <span class="logo__text"
      >John Inacay</span
    >
    <span class="logo__cursor"></span>
  
</a>

      <div class="copyright">
        <span
          >© 2021 Powered by
          <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a></span
        >
        <span
          >Theme created by
          <a href="https://twitter.com/panr" target="_blank" rel="noopener">panr</a></span
        >
      </div>
    
  </div>
</footer>

<script src="https://jinac.github.io/assets/main.js"></script>
<script src="https://jinac.github.io/assets/prism.js"></script>


      
    </div>

    
  </body>
</html>
